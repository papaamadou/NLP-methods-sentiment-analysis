# -*- coding: utf-8 -*-

from nltk import wordpunct_tokenize

wordpunct_tokenize("tout d'abord, nous allons commencer par découper notre text en mots. C'est ce qu'on appelle la tokenisation ou tokenizer en anglais")
['tout', 'd', "'", 'abord', ',', 'nous', 'allons', 'commencer', 'par', 'découper', 'notre', 'text', 'en', 'mots', '.', 'C', "'", 'est', 'ce', 'qu', "'", 'on', 'appelle', 'la', 'tokenisation', 'ou', 'tokenizer', 'en', 'anglais']